# ğŸ¤Ÿ Traducteur de Langue des Signes

SystÃ¨me de reconnaissance et traduction de la langue des signes en temps rÃ©el utilisant l'apprentissage profond et la vision par web-cam sur ordinateur/portable.

## ğŸ“‹ PrÃ©requis

- Python 3.8+
- Webcam fonctionnelle
- GPU recommandÃ© (CUDA) pour l'entraÃ®nement

## ğŸš€ Installation

```bash
# Cloner le repository
git clone https://github.com/votre-nom/traducteur-langue-signes.git
cd traducteur-langue-signes

# CrÃ©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les dÃ©pendances
pip install -r requirements.txt
```

## ğŸ“ Structure du Projet
arborescent
```
traducteur-langue-des-signes/
â”‚
â”œâ”€â”€ donnees/
â”‚   â”œâ”€â”€ brutes/              # VidÃ©os brutes (optionnel)
â”‚   â”œâ”€â”€ points_clefs/        # Points clÃ©s extraits (JSON)
â”‚   â””â”€â”€ preparees/           # DonnÃ©es prÃªtes pour l'entraÃ®nement
â”‚       â”œâ”€â”€ train/
â”‚       â”œâ”€â”€ val/
â”‚       â””â”€â”€ test/
â”‚
â”œâ”€â”€ modeles/
â”‚   â”œâ”€â”€ checkpoints/         # Sauvegardes pendant l'entraÃ®nement
â”‚   â””â”€â”€ production/          # ModÃ¨les finaux optimisÃ©s
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ capture/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ capture_points_clefs.py
â”‚   â”‚
â”‚   â”œâ”€â”€ entrainement/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ train_modele.py
â”‚   â”‚   â””â”€â”€ modele_lstm.py   # Architecture du modÃ¨le
â”‚   â”‚
â”‚   â”œâ”€â”€ prediction/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ traduction_signes.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ preprocessing.py
â”‚       â””â”€â”€ config.py         # Configuration centrale
â”‚
â”œâ”€â”€ logs/                     # Journaux d'entraÃ®nement
â”œâ”€â”€ tests/                    # Tests unitaires
â”œâ”€â”€ notebooks/                # Notebooks Jupyter pour l'exploration
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .gitignore
â”œâ”€â”€ config.yaml              # Configuration du projet
â”œâ”€â”€ main.py                  # Interface en ligne de commande
â””â”€â”€ README.md
```

## ğŸ¯ Utilisation

### 1. Capture de donnÃ©es

```bash
# Capturer des gestes pour crÃ©er votre dataset
python main.py capture --mot "bonjour" --nb-sequences 30
```

### 2. EntraÃ®nement du modÃ¨le

```bash
# EntraÃ®ner le modÃ¨le sur vos donnÃ©es
python main.py train --epochs 100 --batch-size 32
```

### 3. Traduction en temps rÃ©el

```bash
# Lancer la traduction en direct
python main.py predict --modele modeles/production/modele_final.h5
```

## ğŸ› ï¸ Configuration

Modifiez `config.yaml` pour personnaliser:
- Nombre de points clÃ©s dÃ©tectÃ©s
- Longueur des sÃ©quences
- HyperparamÃ¨tres du modÃ¨le
- Seuil de confiance

## ğŸ“Š Dataset

Le systÃ¨me dÃ©tecte:
- 21 points de la main (MediaPipe)
- 33 points du corps (pose)
- 468 points du visage (expressions faciales)

Chaque geste est reprÃ©sentÃ© par une sÃ©quence temporelle de ces points.

## ğŸ§ª Tests

```bash
# ExÃ©cuter les tests
pytest tests/
```

## ğŸ“ˆ Performance

- PrÃ©cision: 95%+ sur vocabulaire de base
- Latence: <100ms en temps rÃ©el
- Support: 50+ mots/signes

## ğŸ¤ Contribution

Les contributions sont les bienvenues !.

## ğŸ“„ Licence

demo-test

## ğŸ‘¥ Auteurs

- Skyincap-demo - [@votre-github](https://github.com/votre-nom)

## ğŸ™ Remerciements

- MediaPipe pour la dÃ©tection des points clÃ©s
- TensorFlow/Keras pour le deep learning
- CommunautÃ© LSF pour les ressources
